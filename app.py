import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
import json
import os

# ==============================
# CONFIG
BASE_URL = "https://masothue.com"
HISTORY_FILE = "history.json"

# ==============================
# FILE HANDLING
def save_json_file(filename, data):
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def load_json_file(filename):
    if os.path.exists(filename):
        with open(filename, "r", encoding="utf-8") as f:
            return json.load(f)
    return []

# ==============================
# CRAWLER
def get_total_pages():
    """
    L·∫•y s·ªë trang t·ªëi ƒëa t·ª´ ph√¢n trang
    """
    try:
        resp = requests.get(f"{BASE_URL}/tra-cuu-ma-so-thue-doanh-nghiep-moi-thanh-lap", timeout=10)
        soup = BeautifulSoup(resp.text, "html.parser")
        page_numbers = [int(span.text) for span in soup.select(".page-numbers") if span.text.isdigit()]
        return max(page_numbers) if page_numbers else 1
    except Exception as e:
        st.error(f"‚ö†Ô∏è Kh√¥ng l·∫•y ƒë∆∞·ª£c s·ªë trang: {e}")
        return 1

def fetch_new_companies():
    """
    Crawl to√†n b·ªô DN m·ªõi th√†nh l·∫≠p (t·∫•t c·∫£ c√°c trang)
    """
    rows = []
    total_pages = get_total_pages()
    for page in range(1, total_pages + 1):
        st.info(f"‚è≥ ƒêang t·∫£i d·ªØ li·ªáu (trang {page}/{total_pages})...")
        url = f"{BASE_URL}/tra-cuu-ma-so-thue-doanh-nghiep-moi-thanh-lap?page={page}"
        try:
            resp = requests.get(url, timeout=10)
            soup = BeautifulSoup(resp.text, "html.parser")
            items = soup.select("div.tax-listing h3 > a")
            for a_tag in items:
                name = a_tag.get_text(strip=True)
                link = BASE_URL + a_tag['href']
                parent_div = a_tag.find_parent("div")
                address_tag = parent_div.find_next("address")
                address = address_tag.get_text(strip=True) if address_tag else ""
                rows.append({
                    "T√™n doanh nghi·ªáp": name,
                    "ƒê·ªãa ch·ªâ": address,
                    "Link": link
                })
        except Exception as e:
            st.error(f"‚ö†Ô∏è L·ªói khi t·∫£i trang {page}: {e}")
    return pd.DataFrame(rows)

# ==============================
# MAIN APP
def main():
    st.title("üìä Tra c·ª©u doanh nghi·ªáp m·ªõi th√†nh l·∫≠p")

    if st.button("üîç Tra c·ª©u"):
        st.session_state["search_results"] = fetch_new_companies()
        # L∆∞u v√†o l·ªãch s·ª≠
        history = load_json_file(HISTORY_FILE)
        history.insert(0, {"T√¨m ki·∫øm": "T·∫•t c·∫£ DN m·ªõi", "S·ªë l∆∞·ª£ng": len(st.session_state["search_results"])})
        save_json_file(HISTORY_FILE, history)
        st.success(f"‚úÖ ƒê√£ t√¨m th·∫•y {len(st.session_state['search_results'])} doanh nghi·ªáp m·ªõi.")

    if "search_results" in st.session_state and not st.session_state["search_results"].empty:
        df = st.session_state["search_results"]
        provinces = ["T·∫•t c·∫£"] + sorted(set([addr.split(",")[-1].strip() for addr in df["ƒê·ªãa ch·ªâ"] if "," in addr]))
        selected_province = st.selectbox("üìç L·ªçc theo t·ªânh/TP", provinces)
        if selected_province != "T·∫•t c·∫£":
            filtered_df = df[df["ƒê·ªãa ch·ªâ"].str.contains(selected_province, case=False, na=False)]
        else:
            filtered_df = df
        st.dataframe(filtered_df, use_container_width=True)

    st.header("‚è±Ô∏è L·ªãch s·ª≠ t√¨m ki·∫øm")
    history = load_json_file(HISTORY_FILE)
    if history:
        hist_df = pd.DataFrame(history)
        st.table(hist_df)
    else:
        st.info("üì≠ Ch∆∞a c√≥ l·ªãch s·ª≠ t√¨m ki·∫øm n√†o.")

# ==============================
# ENTRY POINT
if __name__ == "__main__":
    main()
