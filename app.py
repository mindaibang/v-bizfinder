import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
import json
import os

BASE_URL = "https://masothue.com"
HISTORY_FILE = "history.json"

# ========== File handling ==========
def save_json_file(filename, data):
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def load_json_file(filename):
    if os.path.exists(filename):
        with open(filename, "r", encoding="utf-8") as f:
            return json.load(f)
    return []

# ========== Crawl ==========
def fetch_new_companies(limit_pages=5):
    """
    Crawl doanh nghi·ªáp m·ªõi th√†nh l·∫≠p t·ª´ masothue.com (ƒëi qua t·ªëi ƒëa limit_pages)
    """
    results = []
    page_url = f"{BASE_URL}/tra-cuu-ma-so-thue-doanh-nghiep-moi-thanh-lap"
    pages_crawled = 0

    while page_url and pages_crawled < limit_pages:
        try:
            resp = requests.get(page_url, timeout=10)
            resp.raise_for_status()
            soup = BeautifulSoup(resp.text, "html.parser")

            # L·∫•y c√°c doanh nghi·ªáp
            listings = soup.select(".tax-listing > div")
            for item in listings:
                a_tag = item.find("a")
                address_tag = item.find("address")
                if a_tag and address_tag:
                    name = a_tag.text.strip()
                    link = BASE_URL + a_tag["href"]
                    address = address_tag.text.strip()
                    results.append({
                        "T√™n doanh nghi·ªáp": name,
                        "ƒê·ªãa ch·ªâ": address,
                        "Link": link
                    })

            # T√¨m link trang ti·∫øp theo
            next_link = soup.select_one(".page-numbers.current + li a")
            if next_link:
                page_url = BASE_URL + next_link["href"]
            else:
                page_url = None

            pages_crawled += 1

        except Exception as e:
            st.warning(f"‚ö†Ô∏è L·ªói khi t·∫£i trang {pages_crawled+1}: {e}")
            break

    return pd.DataFrame(results)

# ========== UI ==========
st.title("üìä Tra c·ª©u doanh nghi·ªáp m·ªõi th√†nh l·∫≠p")

if "search_results" not in st.session_state:
    st.session_state["search_results"] = pd.DataFrame()

if st.button("üîç Tra c·ª©u"):
    st.info("‚è≥ ƒêang t·∫£i d·ªØ li·ªáu (5 trang)...")
    df = fetch_new_companies(limit_pages=5)
    if not df.empty:
        st.success(f"‚úÖ ƒê√£ t√¨m th·∫•y {len(df)} doanh nghi·ªáp.")
        st.session_state["search_results"] = df

        # L∆∞u l·ªãch s·ª≠
        history = load_json_file(HISTORY_FILE)
        history.insert(0, {"T√¨m ki·∫øm": "5 trang", "S·ªë l∆∞·ª£ng": len(df)})
        save_json_file(HISTORY_FILE, history[:10])
    else:
        st.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu.")

# Hi·ªÉn th·ªã b·∫£ng + l·ªçc t·ªânh
if not st.session_state["search_results"].empty:
    provinces = ["T·∫•t c·∫£"] + sorted(set([addr.split(",")[-1].strip() for addr in st.session_state["search_results"]["ƒê·ªãa ch·ªâ"]]))
    selected_province = st.selectbox("üìç L·ªçc theo t·ªânh/TP", provinces)
    if selected_province != "T·∫•t c·∫£":
        filtered_df = st.session_state["search_results"][st.session_state["search_results"]["ƒê·ªãa ch·ªâ"].str.contains(selected_province, case=False)]
    else:
        filtered_df = st.session_state["search_results"]
    st.dataframe(filtered_df, use_container_width=True)

# L·ªãch s·ª≠
st.subheader("‚è≥ L·ªãch s·ª≠ t√¨m ki·∫øm")
history = load_json_file(HISTORY_FILE)
if history:
    st.table(pd.DataFrame(history))
else:
    st.info("üì≠ Ch∆∞a c√≥ l·ªãch s·ª≠.")
